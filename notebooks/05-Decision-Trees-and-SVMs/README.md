# Project 05: Decision Trees and Support Vector Machines (SVMs)

## Objective

This project explores two powerful and popular supervised learning models for both regression and classification tasks.
We will first implement a Decision Tree, highlighting its versatility and interpretability.
Then, we will introduce the Support Vector Machine (SVM) as an alternative and highly effective method for classification.

## Key Concepts Demonstrated

* **Decision Tree Regressor**: Using a tree-based model to predict a continuous value (housing prices).
* This demonstrates the model's ability to capture non-linear relationships without the need for feature transformation.
* **Decision Tree Classifier**: Applying the same tree-based logic to a classification problem (liver disease prediction) and visualizing the resulting tree to understand its decision-making rules.
* **Tree Visualization**: Using `plot_tree` to create an intuitive visualization of the classification tree, showcasing one of the key advantages of this modelâ€”its "white-box" nature.
* **Support Vector Machine (SVM) Classifier**: Implementing an SVM to solve the same classification problem.
* This demonstrates a different approach to finding decision boundaries by maximizing the margin between classes.
* **Model Comparison**: Evaluating and comparing the performance of the Decision Tree Classifier and the SVM Classifier on the same dataset to understand their relative strengths.

## Datasets Used

* **Processed Housing Data**: The cleaned and scaled `processed_housing_data.csv` is used for the regression task.
* **Liver Patient Data**: The `liver_patient.csv` dataset is used for the classification tasks to compare the two different models.

## Key Findings

Based on the outputs from the notebook:

* The **Decision Tree Regressor** achieved a low MSE of **0.0232** on the housing price task, performing better than a simple linear model and demonstrating its ability to handle complex feature interactions.
* On the classification task, the **Decision Tree Classifier** achieved a solid accuracy of **64.66%**. Its main advantage is its interpretability, as the tree structure can be visualized to understand its decision logic.
* The **SVM Classifier** performed slightly less accurately on this dataset with **62.93%**. This highlights the important principle of "No Free Lunch" in machine learning: no single model is best for every problem, and it's crucial to test multiple architectures.
